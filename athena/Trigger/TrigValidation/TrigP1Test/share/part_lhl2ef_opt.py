# -- CUT FROM THIS POINT UPWARDS --
# This dump was automatically generated by myopt
# User rabello (Andre DOS ANJOS (PH)) at Mon May 26 14:27:13 2008

# All options should be inside the 'option' dictionary
# The keys being the long name of the options and the
# values, the new default to be attributed.

from RecExConfig.RecFlags import rec
from AthenaCommon.AthenaCommonFlags import athenaCommonFlags as acf

option = {}

# ================================
# 'Data Quality Monitoring' options 
# ================================

# If set, should be the (relative) path to a DQM segment for the Event Filter. This segment should contain a single segment that will be linked in through the highest segment at this trigger level.
option['ef-dqm'] = ''

# If set, should be the (relative) path to a DQM segment for the Level-2 Trigger. This segment should contain a single segment that will be linked in through the highest segment at this trigger level.
option['l2-dqm'] = ''

# ================================
# 'Dataflow' options 
# ================================

# If we should use the real ROS instead of the ROSE
option['use-ros'] = False

# List of directories where to place (SFI/O) output files. If you are using an SFO, the first entry in this list will be also used to place the index files.
option['write-dir'] = []

# Maximum number of events to run the L2SV for (0 means forever
option['max-events'] = 0

# List of data files to preload or the size of the dummy ROBs (for partitions with ROSs/ROSEs) or the dummy event (for partitions with an SFI emulator)
from robhit import filename
option['data'] = filename

# ================================
# 'Farm size' options 
# ================================

# Number of PTs per EF machine. The special value 0 (zero) means one PT per core available on the target machine.
option['pts-per-efd'] = 1

# Number of L2PU's per node. The special value 0 (zero) means one L2PU per core available on the target machine.
option['l2pus-per-node'] = 1

# Number of L2PU worker threads
option['l2pu-worker'] = 1

# ================================
# 'HLT' options 
# ================================

# Defines the L2 HLT Implementation to use. It has to be either a python dictionary (like the default argument), a dictionary that configures an HLTImplementation, or a string, that determines the path to the jobOptions to use. This parameter can also be set to dal.HLTImplementation object that will be used blindly to configure the L2 system at your partition.
# option['l2-hlt'] = {'jobOptionsPath': 'TrigExMTHelloWorld/MTHelloWorldOptions.py'}

# Defines the EF HLT Implementation to use. It has to be either a python dictionary (like the default argument), a dictionary that configures an HLTImplementation, or a string, that determines the path to the jobOptions to use. This parameter can also be set to dal.HLTImplementation object that will be used blindly to configure the EF system at your partition.
#option['ef-hlt'] = option['l2-hlt']

# Defines a set of extra paths that should contain valid installation areas where you have patches for the HLT/Offline software. The order you set is preserved. These areas are attached both to L2 and EF HLT nodes.

option['hlt-extra-path'] = []

# ================================
# 'Machine allocation' options 
# ================================

# This parameter defines the name of a python module that contains a dictionary named "ebef_farm". This dictionary should contain the python farm description of the EB-EF farm as described at the documentation of the function "gen_ebef_segment" at the submodule pm.multinode. If the module contains another variable called "includes", it will be used as the OKS hardware database and will be included in the final generated database
option['ebef-farm'] = 'part_lhfarm'

# This parameter defines the name of a python module that contains a dictionary named "ros_farm". This dictionary should contain the python farm description of the ROS farm as described at the documentation of the function "gen_ros_segment" at the submodule pm.multinode. If the module contains another variable called "includes", it will be used as the OKS hardware database and will be included in the final generated database
option['ros-farm'] = 'part_lhfarm'

# This parameter defines the name of a python module that contains a dictionary named "l2_farm". This dictionary should contain the python farm description of the LVL2 farm as described at the documentation of the function "gen_l2_segment" at the submodule pm.multinode. If the module contains another variable called "includes", it will be used as the OKS hardware database and will be included in the final generated database
option['l2-farm'] = 'part_lhfarm'

# ================================
# 'Partition making' options 
# ================================

# Add a RepositoryRoot to the partition
option['repository-root'] = ''
 
# If you set this option it has to point to python module that contains a function named "modify" that will take the object generated (segment or partition) and post process it using such a function. This function should just modify the required objects and attributes and return the outcome. PartitionMaker will persistify that result. The specification of the module path follows the python standard, separating paths with dots. Any valid python module in PYTHONPATH or sys.path will work. Multiples of this option may be given at the command line in which case the filters are applied sequentially.
option['post-processor'] = []

# Which standard template file to include in your partition. This file contains the RC and RDB templates to be used at your segment or partition.
option['template-file'] = 'daq/sw/hlt-templates.data.xml'

# If this flag is set, the output will not contain any applications, but just the control tree (useful for controller-only tests)
option['control-only'] = False

# The name of the OKS partition
option['partition-name'] = 'part_lhl2ef'

# If this option is set, we will remove all templates from your segment or partition object transforming them in plain non-templated applications, respecting the number of instances of each of those you actually want to run in every machine.
option['no-templates'] = False

# Which setup file to include in your partition
option['setup-file'] = 'daq/segments/setup.data.xml'

# Extra OKS includes to your output
option['extra-includes'] = []
